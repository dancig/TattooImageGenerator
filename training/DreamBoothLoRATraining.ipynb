{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H66qVoQHQYU",
        "outputId": "0c49e23e-86ba-423f-d6cc-52b867de37ce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nWqxhfQvcpHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0e14d6-6220-482c-e1e9-233786fc962f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install xformers bitsandbytes transformers accelerate wandb dadaptation prodigyopt -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/diffusers.git -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YYV9JwwFh3R",
        "outputId": "de05a6bc-54ad-4382-e82b-8112689f24b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/diffusers/main/examples/advanced_diffusion_training/train_dreambooth_lora_sd15_advanced.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HJeU6tzvyHn",
        "outputId": "8b3565b9-2065-492a-c952-6eff5835e406"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-06 17:01:29--  https://raw.githubusercontent.com/huggingface/diffusers/main/examples/advanced_diffusion_training/train_dreambooth_lora_sd15_advanced.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88906 (87K) [text/plain]\n",
            "Saving to: ‘train_dreambooth_lora_sd15_advanced.py’\n",
            "\n",
            "train_dreambooth_lo 100%[===================>]  86.82K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-06-06 17:01:29 (5.76 MB/s) - ‘train_dreambooth_lora_sd15_advanced.py’ saved [88906/88906]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/datasets.git -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0wftIOlF68h",
        "outputId": "4ecb82d7-7365-446d-b6ed-b77ee42cee3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for datasets (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL5SqeQGaGo9",
        "outputId": "2a9f21d3-1642-4489-dcc3-c0d090b90d55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ],
      "source": [
        "!accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aulf8f9NlFLf",
        "outputId": "8cf502cd-9faf-4b65-b761-7a534b12dfea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "The token `colab1` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `colab1`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il2sLhW4beYZ",
        "outputId": "cc2de522-dab8-4f52-bf97-546ccf7f4ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-06 17:08:18.450085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1749229698.482298    3378 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1749229698.492075    3378 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-06-06 17:08:18.522340: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
            "{'rescale_betas_zero_snr', 'thresholding', 'dynamic_thresholding_ratio', 'prediction_type', 'clip_sample_range', 'timestep_spacing', 'sample_max_value', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
            "{'latents_mean', 'use_post_quant_conv', 'latents_std', 'force_upcast', 'shift_factor', 'scaling_factor', 'use_quant_conv', 'mid_block_add_attention'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing AutoencoderKL.\n",
            "\n",
            "All the weights of AutoencoderKL were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use AutoencoderKL for predictions without further training.\n",
            "{'addition_time_embed_dim', 'time_embedding_type', 'cross_attention_norm', 'timestep_post_act', 'dual_cross_attention', 'use_linear_projection', 'num_class_embeds', 'only_cross_attention', 'mid_block_type', 'mid_block_only_cross_attention', 'num_attention_heads', 'conv_out_kernel', 'class_embeddings_concat', 'dropout', 'class_embed_type', 'attention_type', 'encoder_hid_dim', 'time_embedding_dim', 'time_embedding_act_fn', 'resnet_skip_time_act', 'reverse_transformer_layers_per_block', 'projection_class_embeddings_input_dim', 'addition_embed_type', 'conv_in_kernel', 'addition_embed_type_num_heads', 'upcast_attention', 'transformer_layers_per_block', 'encoder_hid_dim_type', 'resnet_out_scale_factor', 'time_cond_proj_dim', 'resnet_time_scale_shift'} was not found in config. Values will be initialized to default values.\n",
            "All model checkpoint weights were used when initializing UNet2DConditionModel.\n",
            "\n",
            "All the weights of UNet2DConditionModel were initialized from the model checkpoint at stable-diffusion-v1-5/stable-diffusion-v1-5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use UNet2DConditionModel for predictions without further training.\n",
            "Resolving data files: 100% 51/51 [00:00<00:00, 162668.82it/s]\n",
            "Downloading data: 100% 51/51 [00:00<00:00, 102202.34files/s]\n",
            "Generating train split: 50 examples [00:00, 1256.76 examples/s]\n",
            "validation prompt: None\n",
            "Steps: 100% 1500/1500 [1:17:14<00:00,  3.09s/it, loss=0.0332, lr=0]     Model weights saved in /content/drive/MyDrive/lora_output/pytorch_lora_weights.safetensors\n",
            "Steps: 100% 1500/1500 [1:17:14<00:00,  3.09s/it, loss=0.0332, lr=0]\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch train_dreambooth_lora_sd15_advanced.py \\\n",
        "  --pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\" \\\n",
        "  --dataset_name=\"/content/drive/MyDrive/dataset\" \\\n",
        "  --instance_prompt=\"Tattoosketch\" \\\n",
        "  --output_dir=\"/content/drive/MyDrive/lora_output\" \\\n",
        "  --caption_column=\"prompt\" \\\n",
        "  --mixed_precision=\"bf16\" \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=2 \\\n",
        "  --repeats=10 \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --gradient_checkpointing \\\n",
        "  --learning_rate=5e-4 \\\n",
        "  --text_encoder_lr=1e-4 \\\n",
        "  --optimizer=\"AdamW\"\\\n",
        "  --snr_gamma=5.0 \\\n",
        "  --lr_scheduler=\"cosine_with_restarts\" \\\n",
        "  --lr_num_cycles=3 \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --rank=\"16\" \\\n",
        "  --max_train_steps=1500 \\\n",
        "  --checkpointing_steps=2000 \\\n",
        "  --seed=\"517\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}